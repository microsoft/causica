{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example notebook showing how to train a [Rhino](https://arxiv.org/abs/2210.14706) model on Ecoli Data.\n",
    "\n",
    "This demonstrates how to assemble the various components of the library and how to perform training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from tensordict import TensorDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from causica.datasets.causica_dataset_format import CAUSICA_DATASETS_PATH, DataEnum, load_data\n",
    "from causica.datasets.tensordict_utils import (\n",
    "    tensordict_shapes,\n",
    ")\n",
    "from causica.datasets.timeseries_dataset import IndexedTimeseriesDataset\n",
    "from causica.distributions import (\n",
    "    AdjacencyDistribution,\n",
    "    ContinuousNoiseDist,\n",
    "    DistributionModule,\n",
    "    ENCOAdjacencyDistributionModule,\n",
    "    GibbsDAGPrior,\n",
    "    JointNoiseModule,\n",
    "    create_noise_modules,\n",
    "    RhinoLaggedAdjacencyDistributionModule,\n",
    "    TemporalAdjacencyDistributionModule,\n",
    ")\n",
    "from causica.datasets.variable_types import VariableTypeEnum\n",
    "from causica.functional_relationships import TemporalEmbedFunctionalRelationships\n",
    "from causica.graph.dag_constraint import calculate_dagness\n",
    "from causica.sem.sem_distribution import TemporalSEMDistributionModule\n",
    "from causica.sem.temporal_distribution_parameters_sem import (\n",
    "    concatenate_lagged_and_instaneous_values,\n",
    "    split_lagged_and_instanteneous_values,\n",
    ")\n",
    "from causica.training.auglag import AugLagLossCalculator, AugLagLR, AugLagLRConfig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define various parameters of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"https://azuastoragepublic.z6.web.core.windows.net/Ecoli1_100/train.csv\"\n",
    "adj_path = \"https://azuastoragepublic.z6.web.core.windows.net/Ecoli1_100/adj_matrix.npy\"\n",
    "\n",
    "device = \"cpu\"\n",
    "dataset_train = IndexedTimeseriesDataset(series_index_key=0, data=data_path, adjacency_matrix=adj_path, device=device)\n",
    "dataloader_train = DataLoader(\n",
    "    dataset=dataset_train,\n",
    "    collate_fn=torch.stack,\n",
    "    batch_size=int(os.environ.get(\"TEST_RUN\", 8)),\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(dataset_train._data.keys())\n",
    "context_length = 21\n",
    "lags = 21 - 1\n",
    "\n",
    "prior = GibbsDAGPrior(num_nodes=num_nodes, sparsity_lambda=0.3, context_length=context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Variational Posterior Distribution over Adjacency Matrices, which we will optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_dist: DistributionModule[AdjacencyDistribution] = TemporalAdjacencyDistributionModule(\n",
    "    ENCOAdjacencyDistributionModule(num_nodes),\n",
    "    RhinoLaggedAdjacencyDistributionModule(num_nodes, lags),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Graph Neural network that will estimate the functional relationships. More info can be found [here](https://openreview.net/forum?id=S2pNPZM-w-f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_relationships = TemporalEmbedFunctionalRelationships(\n",
    "    shapes=tensordict_shapes(dataset_train._data),\n",
    "    embedding_size=32,\n",
    "    out_dim_g=32,\n",
    "    num_layers_g=2,\n",
    "    num_layers_zeta=2,\n",
    "    context_length=context_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Noise Distributions for each node assuming they are all continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_shapes = tensordict_shapes(dataset_train._data)\n",
    "types_dict = {key: VariableTypeEnum.CONTINUOUS for key in dataset_train._data.keys()}\n",
    "\n",
    "noise_submodules = create_noise_modules(variable_shapes, types_dict, ContinuousNoiseDist.GAUSSIAN)\n",
    "noise_module = JointNoiseModule(noise_submodules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the SEM Module which combines the variational adjacency distribution, the functional relationships and the noise distributions for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_module: TemporalSEMDistributionModule = TemporalSEMDistributionModule(\n",
    "    adjacency_dist, functional_relationships, noise_module\n",
    ")\n",
    "sem_module = sem_module.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the [Augmented Lagrangian Scheduler](https://en.wikipedia.org/wiki/Augmented_Lagrangian_method).\n",
    "\n",
    "This allows Rhino to optimize towards a DAG, by slowly increasing the alpha and rho parameters as the optimization takes\n",
    "place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_init_dict = {\n",
    "    \"functional_relationships\": 3e-3,\n",
    "    \"vardist_inst\": 1e-2,\n",
    "    \"vardist_lagged\": 1e-2,\n",
    "    \"noise_dist\": 3e-4,\n",
    "}\n",
    "\n",
    "auglag_config = AugLagLRConfig(lr_init_dict=lr_init_dict)\n",
    "scheduler = AugLagLR(config=auglag_config)\n",
    "auglag_loss = AugLagLossCalculator(init_alpha=0.0, init_rho=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the optimizer, with separate learning rates for each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = {\n",
    "    \"functional_relationships\": sem_module.functional_relationships,\n",
    "    \"vardist_inst\": sem_module.adjacency_module.inst_dist_module,\n",
    "    \"vardist_lagged\": sem_module.adjacency_module.lagged_dist_module,\n",
    "    \"noise_dist\": sem_module.noise_module,\n",
    "}\n",
    "\n",
    "parameter_list = [\n",
    "    {\"params\": module.parameters(), \"lr\": auglag_config.lr_init_dict[name], \"name\": name}\n",
    "    for name, module in modules.items()\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam(parameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training loop.\n",
    "\n",
    "For each batch, we:\n",
    "* Sample a graph from the SEM.\n",
    "* Calculate the log probability of that batch, given the graph.\n",
    "* Create the ELBO to be optimized.\n",
    "* Calculate the DAG constraint\n",
    "* Combine the DAG constraint with the ELBO to get the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 loss:2.3792e+17 nll:123.08 dagness:31731050496.00000 num_edges:102465 alpha:0 rho:1 step:0|1 num_lr_updates:0\n",
      "\n",
      "epoch:10 loss:2.1545e+17 nll:90.606 dagness:30195685376.00000 num_edges:96460 alpha:0 rho:1 step:0|51 num_lr_updates:0\n"
     ]
    }
   ],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    noise_dist: ContinuousNoiseDist = ContinuousNoiseDist.SPLINE\n",
    "    batch_size: int = int(os.environ.get(\"TEST_RUN\", 128))\n",
    "    max_epoch: int = int(os.environ.get(\"TEST_RUN\", 20))  # used by testing to run the notebook as a script\n",
    "    gumbel_temp: float = 0.25\n",
    "    averaging_period: int = 10\n",
    "    prior_sparsity_lambda: float = 5.0\n",
    "    init_rho: float = 1.0\n",
    "    init_alpha: float = 0.0\n",
    "\n",
    "\n",
    "training_config = TrainingConfig()\n",
    "\n",
    "num_samples = len(dataset_train)\n",
    "for epoch in range(training_config.max_epoch):\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        batch = batch.to_tensordict()  # Force dense stacking for tensordict<0.4.0\n",
    "        batch.batch_size = batch.batch_size[:1]  # Do not consider the time axis part of the batch dims\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sem_distribution = sem_module()\n",
    "        sem, *_ = sem_distribution.relaxed_sample(\n",
    "            torch.Size([]), temperature=training_config.gumbel_temp\n",
    "        )  # soft sample\n",
    "        graph_tries = 0\n",
    "        while sem.graph.isnan().any() and graph_tries < 2:\n",
    "            sem, *_ = sem_distribution.relaxed_sample(torch.Size([]), temperature=training_config.gumbel_temp)\n",
    "            graph_tries += 1\n",
    "        if sem.graph.isnan().any():\n",
    "            raise ValueError(f\"Failed to sample a valid graph after {graph_tries} tries\")\n",
    "        if graph_tries > 0:\n",
    "            print(f\"Used {graph_tries} tries to sample a valid graph\")\n",
    "\n",
    "        batch_log_prob = sem.log_prob(batch).mean()\n",
    "        sem_distribution_entropy = sem_distribution.entropy()\n",
    "        prior_term = prior.log_prob(sem.graph)\n",
    "        objective = (-sem_distribution_entropy - prior_term) / num_samples - batch_log_prob\n",
    "        constraint = calculate_dagness(sem.graph[..., -1, :, :])  # Calculate dagness on instantaneous graph\n",
    "\n",
    "        loss = auglag_loss(objective, constraint / num_samples)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # update the Auglag parameters\n",
    "        scheduler.step(\n",
    "            optimizer=optimizer,\n",
    "            loss=auglag_loss,\n",
    "            loss_value=loss,\n",
    "            lagrangian_penalty=constraint,\n",
    "        )\n",
    "        # log metrics\n",
    "        if epoch % 10 == 0 and i == 0:\n",
    "            print(\n",
    "                f\"epoch:{epoch} loss:{loss.item():.5g} nll:{-batch_log_prob.detach().cpu().numpy():.5g} \"\n",
    "                f\"dagness:{constraint.item():.5f} num_edges:{(sem.graph > 0.0).sum()} \"\n",
    "                f\"alpha:{auglag_loss.alpha:.5g} rho:{auglag_loss.rho:.5g} \"\n",
    "                f\"step:{scheduler.outer_opt_counter}|{scheduler.step_counter} \"\n",
    "                f\"num_lr_updates:{scheduler.num_lr_updates}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantaneous nans: 1 Lagged nans: 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The above used to throw errors because of NaNs in the lagged graph.\n",
    "# This is an ephemeral issue that is not present in the final model.\n",
    "inst_nans = np.sum(\n",
    "    [\n",
    "        sem_module.adjacency_module.inst_dist_module()\n",
    "        .relaxed_sample(temperature=training_config.gumbel_temp)\n",
    "        .isnan()\n",
    "        .any()\n",
    "        for _ in range(1000)\n",
    "    ]\n",
    ")\n",
    "lagged_nans = np.sum(\n",
    "    [\n",
    "        sem_module.adjacency_module.lagged_dist_module()\n",
    "        .relaxed_sample(temperature=training_config.gumbel_temp)\n",
    "        .isnan()\n",
    "        .any()\n",
    "        for _ in range(1000)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Instantaneous nans: {inst_nans} Lagged nans: {lagged_nans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([8, 21, 1]), torch.Size([8])\n",
      "\n",
      "Sample to noise shape: torch.Size([8, 1])\n",
      "Sampled noise shape: torch.Size([1])\n",
      "Generated sample shape: torch.Size([8, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"batch shape: {batch[batch.sorted_keys[0]].shape}, {batch.batch_size}\")\n",
    "# Sample to noise\n",
    "noise = sem.sample_to_noise(batch)\n",
    "print(f\"Sample to noise shape: {noise[noise.sorted_keys[0]].shape}\")\n",
    "\n",
    "# Sampled noise\n",
    "sampled_noise = sem.sample_noise()\n",
    "print(f\"Sampled noise shape: {sampled_noise[sampled_noise.sorted_keys[0]].shape}\")\n",
    "\n",
    "# Noise to sample still fails as it requires the history:\n",
    "# sem.noise_to_sample(noise)\n",
    "\n",
    "history, inst = split_lagged_and_instanteneous_values(batch)\n",
    "concat_noise = concatenate_lagged_and_instaneous_values(history, noise)\n",
    "\n",
    "new_sample = sem.noise_to_sample(concat_noise)\n",
    "print(f\"Generated sample shape: {new_sample[new_sample.sorted_keys[0]].shape}\")\n",
    "# Because sampling requires the history sem.sample() will fail as the history is not provided\n",
    "\n",
    "intervention_variable = sem.node_names[0]\n",
    "intervention = TensorDict({intervention_variable: torch.zeros_like(batch[intervention_variable][0])}, batch_size=[])\n",
    "# Interventions currently faily because we are intervening on the instanteneous values but want to use the history for\n",
    "# forward predictions.\n",
    "# do_sem = sem.do(intervention)\n",
    "# do_sample = do_sem.noise_to_sample(concat_noise.clone())\n",
    "\n",
    "# print(f\"Generated sample after intervention shape: {do_sample[do_sample.sorted_keys[0]].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
