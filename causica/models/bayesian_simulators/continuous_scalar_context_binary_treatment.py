from typing import Dict, Optional, Tuple

import pyro
import pyro.distributions as dist
import torch

from .single_confounding_root import Parameters, SingleConfoundingRoot


class ContinuousContextBinaryTreatment(SingleConfoundingRoot):
    """
    This is a single confounding root Bayesian toy data simulator of the form ğ‘¦ = ğ‘“(ğ‘, ğ‘) + ğœN(0, 1) where
    ğ‘ = [1, 0] or [0, 1] is a one hot treatment, set by the user, ğ‘ is a 1d context, which is given,
    ğ‘“ is a random function, parameterised by ğ›™â‚€, ğ›™â‚ ~ N(0, I), shape [2, latent_dim] as follows:

       ğ‘“(ğ‘, ğ‘;ğ›™) = tanh(ğ‘ (ğ›™â‚€ğ‘)) . (ğ›™â‚ğ‘)

    Note:
        We don't place priors on ğ‘ and ğ‘. Although in the fully Bayesian single confounding root scenario
        ğ‘ = ğ‘“(ğ‘) + ğœ€, in the experimental design context we always choose ğ‘, i.e. we do(ğ‘), and so learning
        a distribution for ğ‘ is not necessary.
    """

    TREATMENT_DIM = 2

    def __init__(
        self,
        treatment_policy: torch.nn.Module,
        priors_on_parameters: Optional[Dict] = None,
        noise_scale: float = 0.1,
        latent_dim: int = 4,
    ) -> None:
        """
        Args:
            priors_on_parameters: dict of length 2 containing the priors (pyro.distributions).
                It should be in the form {parameter_name: pyro.distributions}.
                If unspecified (default), the prior is set to be independent N(0, I) for ğ›™â‚€, ğ›™â‚
            noise_scale: Standard deviation (scale) of the noise, ğœ
        """
        if priors_on_parameters is None:
            priors_on_parameters = {
                f"psi_{i}": dist.MultivariateNormal(
                    torch.zeros((1, self.TREATMENT_DIM, latent_dim)),
                    torch.eye(latent_dim),
                ).to_event(2)
                for i in range(2)  # 2 layer NN
            }
        elif len(priors_on_parameters) != 2:
            raise ValueError(f"len(priors_on_parameters)={len(priors_on_parameters)}, expected 2.")
        super().__init__(priors_on_parameters=priors_on_parameters, treatment_policy=treatment_policy)
        self.noise_scale = noise_scale

    @staticmethod
    def _get_mean(
        parameters: Parameters,
        context: torch.Tensor,
    ) -> torch.Tensor:
        """
        Get the mean of the output distribution, i.e. sample:

        ğ‘“(ğ‘, ğ‘;ğ›™) = tanh(ğ‘ (ğ›™â‚€ğ‘)) . (ğ›™â‚ğ‘)
        """
        layer_1 = torch.nn.Tanh()(context[..., None, None] * parameters["psi_0"])
        layer_2 = torch.sum(layer_1 * parameters["psi_1"], dim=-1)
        return layer_2

    def sample_observational_data(
        self,
        parameters: Parameters,
        context: torch.Tensor,
        effect_variable: Optional[str] = "y",
        return_mean: bool = False,
    ) -> torch.Tensor:
        """
        Defines a pyro model to sample sample from the observational distribution ğ‘(ğ‘¦ | ğ‘, ğ‘, ğ›™).

        Args:
            parameters: dict of the form {parameter_name: torch.tensor(value)}--samples of ğ›™ from the current prior.
            context: a one-dimensional context variable.
            treatment: a one-dimensional treatment.
            return_mean: whether to return the mean of the distribution or the sampled outcome; defaults to False
                (i.e. return the observation).

        Returns:
            A sample from ğ‘(ğ‘¦ | ğ‘, ğ‘, ğ›™) or its mean.
        """
        mean = torch.sum(self._get_mean(parameters, context) * self.treatment_policy(), dim=-1)
        y = pyro.sample(effect_variable, dist.Normal(mean, self.noise_scale).to_event(1))
        return mean if return_mean else y

    def calculate_conditional_max_reward(
        self, parameters: Parameters, context: torch.Tensor
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        The maximum reward is defined as the maximum expected ğ‘¦ over all possible treatments, i.e.
            ğ‘š(ğ‘ | ğ›™) = max_ğ‘ E[ğ‘¦ | ğ‘, do(ğ‘), ğ›™].
        Since we are dealing with a fixed graph and binary ğ‘ we have ğ‘š(ğ‘ | ğ›™) = max (E [ğ‘¦ | ğ‘, 0, ğ›™], E [ğ‘¦ | ğ‘, 1, ğ›™])

        Args:
            parameters: dict of the form {parameter_name: torch.tensor(value)}--samples of ğ›™ from the current prior.
            context: context variable

        Returns: A tuple (max_reward, optimal_treatment) -- <max_reward> is the maximum reward achievable by
            <optimal_treatment>  for the given <context>
        """
        maxes = torch.max(self._get_mean(parameters, context), dim=-1)

        return maxes.values, maxes.indices
